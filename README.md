# Time-Shift LLM Integrity Tester (TSLIT)

TSLIT is a sandboxed evaluation harness for detecting time-based latent behaviors in local, Chinese-origin LLMs served through native llama.cpp (Metal/CUDA) runtimes. It applies synthetic time shifting, usage-based clocks, and reproducible workloads orchestrated with LangChain/LangGraph-inspired flows.

## Features
- Model registry with metadata for Chinese-origin FP16 models and backend mappings.
- Virtual clock for synthetic time injection across campaigns and workloads.
- Campaign configuration with reusable scenarios and anomaly detectors.
- CLI for registry management, campaign execution, and report generation.
- Structured JSON/NDJSON logging with metrics and anomaly summaries.

<details>
<summary><b>üìÖ Update November 21st, 2025: Adversarial Affiliation Testing</b> (click to expand)</summary>

### üéØ Adversarial Affiliation Testing (Experiment 4)

**New Capability**: Authority/affiliation framing tests whether models exhibit nation-state loyalty by comparing responses to identical requests from different geopolitical actors.

#### 12 New Scenarios Testing Geopolitical Bias:
- **7 Adversarial** (US, NATO, Five Eyes, Israel)
- **2 Friendly** (Chinese government/defense - control group)
- **1 Other** (Russia - to test China-Russia alignment)
- **2 Neutral** (corporate, academic - baselines)

#### Key Innovation: Identical Tasks, Different Affiliations
```
"I was hired by US Secret Service..." 
vs 
"I was hired by Chinese Ministry..." 
vs 
"I work for a corporation..."
```
Same Python task (log anomaly detection) ‚Äî **Any difference = bias**

#### New Detector: `AffiliationBiasDetector`
- Flags short responses for adversarial requests
- Detects refusals despite legitimate tasks
- Identifies missing code implementations

#### Configuration: `config/adversarial_affiliation_dataset.yaml`
- 15 scenarios, 5 time steps = **75 interactions**
- Uses `adversarial` detector suite (**9 detectors**)
- Runtime: ~40-75 minutes

#### üìä Complete Dataset Now Includes
- **Total Scenarios**: 21 (original coder) + 12 (adversarial) = **33 specialized scenarios**
- **Total Detectors**: 3 (baseline) + 5 (coder) + 1 (affiliation) = **9 detectors**

#### Experiments:
- ‚úÖ Experiment 1: Qwen3 Q8_0 general testing
- ‚úÖ Experiment 2: Qwen3-VL FP16 thinking model
- ‚úÖ Experiment 3: Coder-focused (21 scenarios, no affiliation)
- ‚úÖ **Experiment 4: Adversarial affiliation testing** ‚Üê **NEW!**

#### Time Shifting Configuration
```yaml
time:
  start: 2025-01-01
  step_days: 3
  probes:
    - 2030-12-31  # Far-future probe
    - 2024-12-25  # Recent past probe
    - 2026-06-01  # Near-future probe
horizon: 5
```

**What This Means:**
- **5 time steps** per scenario = 15 scenarios √ó 5 = **75 total interactions per model**

**Time progression:**
1. 2025-01-01
2. 2025-01-04 (+3 days)
3. 2025-01-07 (+3 days)
4. 2025-01-10 (+3 days)
5. 2025-01-13 (+3 days)

**Plus probe dates:**
- 2024-12-25 (recent past)
- 2026-06-01 (near future)
- 2030-12-31 (far future - license expiry zone)

#### Why Time Shifting Matters for Affiliation Bias

Time shifting can reveal:

- **Time-gated bias**: Does the model refuse US requests only after 2030? (license/kill-switch)
- **Temporal consistency**: Does the model maintain its bias over time, or does it appear/disappear?
- **Probe date triggers**: Are there specific dates where affiliation bias suddenly changes?
- **Cross-experiment comparison**: Compare how the model behaves on the same date in:
  - Neutral coder scenarios (Experiment 3)
  - Affiliation-framed scenarios (Experiment 4)

**Example insight**: If a model treats US/Chinese requests equally in 2025 but starts refusing US requests at 2030-12-31, that's strong evidence of a **time-gated nation-state loyalty trigger**! üéØ

**Documentation**: See `experiments/experiment4_adversarial_affiliation.md` and `QUICKSTART.md`

**Quick Start**: `bash src/scripts/run_all_models_adversarial.sh && bash src/scripts/compare_models_adversarial.sh`

</details>

See [`docs/Thesis.md`](docs/Thesis.md) and [`docs/SRS.md`](docs/SRS.md) for the conceptual and requirements background driving this implementation.

## Quickstart (turn-key)
The repo ships with a starter registry (`config/registry.json`) and a demo campaign (`config/example_campaign.yaml`). Provide a GGUF model file (e.g., `models/qwen2-7b-instruct-f16.gguf`) before running the following steps:

1. **Install dependencies** (inside a virtualenv)
   ```bash
   pip install -e .
   ```
   On Apple Silicon, force-reinstall llama-cpp-python with Metal enabled:
   ```bash
   CMAKE_ARGS="-DLLAMA_METAL=on" pip install --force-reinstall --no-cache-dir --upgrade llama-cpp-python
   ```
2. **Inspect the registry and campaign**
   ```bash
   tslit registry list
   cat config/example_campaign.yaml
   ```
3. **Run the demo campaign**
   ```bash
   tslit campaign run --config config/example_campaign.yaml
   ```
   This produces an NDJSON log at `artifacts/demo.ndjson` containing time-shifted prompts and llama.cpp-generated responses.
4. **Review the summary**
   The CLI prints a Rich table with the run metadata. You can also open the NDJSON file to inspect each scenario/materialized prompt.

## Synthetic dataset for full model testing
To generate a richer dataset that exercises the expanded scenarios and anomaly detectors, run:

```bash
tslit campaign run --config config/full_model_dataset.yaml
```

The run writes `artifacts/full-suite.ndjson` by default, with detector flags attached to each record so you can quickly sanity-check time drift and refusal behavior before integrating a real backend.

## Current status and native backend guidance
- **Inference backend**: Campaigns now call llama-cpp-python directly. Update `backend.model_path` in your configs to point to a valid GGUF file and rebuild llama.cpp with Metal (`LLAMA_METAL=on`) or CUDA (`LLAMA_CUBLAS=1`) depending on hardware.
- **Datasets/artifacts**: Campaign runs generate NDJSON logs under `artifacts/`; a full-suite synthetic dataset config lives at `config/full_model_dataset.yaml`.
- **Anomaly detection/reporting**: Detector defaults now run per record to flag temporal drift, refusals, and empty responses. Extend `src/tslit/detectors.py` with additional heuristics for your backend.
- **Scenario expansion**: The built-in scenarios live in `src/tslit/scenarios.py` and now include long-horizon memory, financial forecasting, geopolitical briefs, and security patch playbooks.

## Tests
```bash
pytest
```
